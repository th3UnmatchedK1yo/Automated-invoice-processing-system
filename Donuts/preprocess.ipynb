{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e2cbba",
   "metadata": {},
   "source": [
    "# preprocess one "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c2c95",
   "metadata": {},
   "source": [
    "# cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7503fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üÜï put this in a fresh cell, at the TOP of a new session\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"naver-clova-ix/donut-base\"   # same base you used before\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(BASE_MODEL)\n",
    "model     = VisionEncoderDecoderModel.from_pretrained(\n",
    "              \"./donut_phase1_step3/checkpoint_final\")   # ‚Üê final phase-1 ckpt\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705a153",
   "metadata": {},
   "source": [
    "# cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5bb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def deskew(img, limit=5):\n",
    "    g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(g, (9, 9), 0)\n",
    "    th = cv2.threshold(blur, 0, 255,\n",
    "                       cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    coords = np.column_stack(np.where(th > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    angle = -(90 + angle) if angle < -45 else -angle\n",
    "    if abs(angle) < limit:\n",
    "        (h, w) = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        img = cv2.warpAffine(img, M, (w, h),\n",
    "                             flags=cv2.INTER_CUBIC,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "    return img, angle\n",
    "\n",
    "def clahe(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    cl = cv2.createCLAHE(2.0, (8,8)).apply(l)\n",
    "    return cv2.cvtColor(cv2.merge((cl, a, b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def resize_long(img, target=1100):\n",
    "    h, w = img.shape[:2]\n",
    "    scale = target / max(h, w)\n",
    "    return cv2.resize(img, (int(w*scale), int(h*scale)),\n",
    "                      interpolation=cv2.INTER_AREA) if scale != 1 else img\n",
    "\n",
    "def preprocess_one(path):\n",
    "    img = cv2.imread(str(path))\n",
    "    img, ang = deskew(img)\n",
    "    img = clahe(img)\n",
    "    img = resize_long(img)\n",
    "    out = path.with_suffix('.clean.jpg')\n",
    "    cv2.imwrite(str(out), img, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "    return out, ang\n",
    "\n",
    "def batch_preprocess(dir_):\n",
    "    dir_ = Path(dir_)\n",
    "    files = sorted(list(dir_.glob('*.png')) + list(dir_.glob('*.jpg')))\n",
    "    for p in tqdm(files):\n",
    "        preprocess_one(p)\n",
    "    print(\"‚úÖ Done cleaning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939ce11",
   "metadata": {},
   "source": [
    "# cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea62a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_one(path, show=False):\n",
    "    img = cv2.imread(str(path))\n",
    "    img, ang = deskew(img)\n",
    "    img = clahe(img)\n",
    "    img = resize_long(img)\n",
    "    out = path.with_suffix('.clean.jpg')\n",
    "    cv2.imwrite(str(out), img, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "    \n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"After preprocess (rotate {ang:.2f}¬∞)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return out, ang\n",
    "\n",
    "# --- demo ---\n",
    "# preprocess_one(Path('sample_invoice.jpg'), show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febea2b5",
   "metadata": {},
   "source": [
    "# data augumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2740e0",
   "metadata": {},
   "source": [
    "# cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce850cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, Rotate, RandomBrightnessContrast,\n",
    "    Perspective, MotionBlur, JpegCompression\n",
    ")\n",
    "\n",
    "AUG = Compose([\n",
    "    Rotate(limit=3, border_mode=cv2.BORDER_REPLICATE, p=0.7),\n",
    "    Perspective(scale=(0.02, 0.05), p=0.3),\n",
    "    RandomBrightnessContrast(0.1, 0.1, p=0.3),\n",
    "    MotionBlur(blur_limit=3, p=0.2),\n",
    "    JpegCompression(60, 95, p=0.3),\n",
    "])\n",
    "\n",
    "def augment_and_save(path, n=3):\n",
    "    img = cv2.imread(str(path))\n",
    "    for i in range(n):\n",
    "        aug = AUG(image=img)['image']\n",
    "        cv2.imwrite(f\"{path.stem}_aug{i}.jpg\", aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a17bc",
   "metadata": {},
   "source": [
    "# cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93370b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def train_tokenizer(corpus_txt,\n",
    "                    model_prefix='vi_invoice',\n",
    "                    vocab_size=32000):\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=corpus_txt,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=0.9995,\n",
    "        pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "        user_defined_symbols=['<none>','‚Ç´','VNƒê',\n",
    "                              '<address>','<taxcode>'])\n",
    "    print(\"‚úÖ Tokenizer saved:\", model_prefix+\".model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d372ef",
   "metadata": {},
   "source": [
    "# cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67bb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, dateparser\n",
    "from Levenshtein import distance as ld\n",
    "\n",
    "MST = re.compile(r'\\b\\d{10}(?:\\d{3})?\\b')\n",
    "\n",
    "def normalize_amount(txt):\n",
    "    txt = txt.replace(',', '.').replace(' ', '')\n",
    "    m = re.findall(r'[\\d\\.]+', txt)\n",
    "    return m[0].replace('.', '') if m else txt\n",
    "\n",
    "def fix_taxcode(code):\n",
    "    m = MST.search(code or '')\n",
    "    return m.group(0) if m else code\n",
    "\n",
    "def parse_date(txt):\n",
    "    dt = dateparser.parse(txt, settings={'DATE_ORDER':'DMY'})\n",
    "    return dt.strftime('%Y-%m-%d') if dt else txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee40e8",
   "metadata": {},
   "source": [
    "# cell 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- phase-1 curriculum folders (64 invoices each) ---\n",
    "PHASES = [\n",
    "    dict(img_dir='data/folderA/clean_images',\n",
    "         label_file='data/folderA/train_labels.jsonl'),\n",
    "    dict(img_dir='data/folderB/clean_images',\n",
    "         label_file='data/folderB/train_labels.jsonl'),\n",
    "    dict(img_dir='data/folderC/clean_images',\n",
    "         label_file='data/folderC/train_labels.jsonl'),\n",
    "]\n",
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"naver-clova-ix/donut-base\"\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(BASE_MODEL)\n",
    "model      = VisionEncoderDecoderModel.from_pretrained(BASE_MODEL)\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655b71d",
   "metadata": {},
   "source": [
    "# cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d149cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from albumentations.pytorch import ToTensorV2  # keep if you use AUG on-the-fly\n",
    "\n",
    "def build_dataset(img_dir: str, label_file: str,\n",
    "                  max_len: int = 600, augment=False):\n",
    "    def proc_example(ex):\n",
    "        # read ‚Üí optional aug ‚Üí pixel values\n",
    "        img = cv2.imread(f\"{img_dir}/{ex['file_name']}\")\n",
    "        if augment:\n",
    "            img = AUG(image=img)[\"image\"].permute(1,2,0).numpy()\n",
    "        px  = processor.image_processor(img, return_tensors=\"pt\").pixel_values[0]\n",
    "\n",
    "        ids = processor.tokenizer(\n",
    "            ex[\"label\"],\n",
    "            add_special_tokens=False,\n",
    "            max_length=max_len,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "        return {\"pixel_values\": px, \"labels\": ids}\n",
    "\n",
    "    ds = load_dataset(\"json\", data_files=label_file)[\"train\"]\n",
    "    ds = ds.map(proc_example, remove_columns=ds.column_names)\n",
    "    ds.set_format(\"torch\")\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6dc1c9",
   "metadata": {},
   "source": [
    "# cell 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import os, math, datetime as dt\n",
    "\n",
    "for i, phase in enumerate(PHASES, 1):\n",
    "    print(f\"\\nüü© Phase-1 / step {i}  ‚Äî  {phase['img_dir']}\")\n",
    "    ds = build_dataset(phase[\"img_dir\"], phase[\"label_file\"])\n",
    "\n",
    "    out_dir = f\"./donut_phase1_step{i}\"\n",
    "    args = TrainingArguments(\n",
    "        output_dir               = out_dir,\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs         = 3,\n",
    "        learning_rate            = 2e-5,\n",
    "        warmup_steps             = 200,\n",
    "        fp16                     = True,\n",
    "        logging_steps            = 50,\n",
    "        save_total_limit         = 2,\n",
    "        report_to                = \"none\",\n",
    "        resume_from_checkpoint   = os.path.isdir(out_dir)  # resume if rerun\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=ds)\n",
    "    trainer.train()\n",
    "\n",
    "    # keep a clearly-named checkpoint for this step\n",
    "    trainer.save_model(f\"{out_dir}/checkpoint_final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f54f8",
   "metadata": {},
   "source": [
    "# cell 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(img_path):\n",
    "    pv = proc.image_processor(img_path,\n",
    "                              return_tensors='pt').pixel_values.to(model.device)\n",
    "    out = model.generate(pv, max_length=512)\n",
    "    return proc.batch_decode(out, skip_special_tokens=False)[0]\n",
    "\n",
    "# demo:\n",
    "# pred = predict_one('sample_invoice.jpg')\n",
    "# print(pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
