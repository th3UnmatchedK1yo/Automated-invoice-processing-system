{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df173d2",
   "metadata": {},
   "source": [
    "# 0 Â· Environment & base paths â€” run first ðŸ”µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8880d29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    2560,\n",
      "    1920\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 10\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57525\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------- 0. Imports & constants -----------------\n",
    "from pathlib import Path\n",
    "import os, cv2, numpy as np, torch, re, dateparser, matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from albumentations import (Compose, Rotate, RandomBrightnessContrast,\n",
    "                            Perspective, MotionBlur, )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from datasets import load_dataset\n",
    "from transformers import (DonutProcessor, VisionEncoderDecoderModel,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "# ----- curriculum folders (Phase-1: 3 Ã— 64 invoices) -----\n",
    "PHASES = [\n",
    "    dict(img_dir='/mnt/c/Users/Legion/Documents/jimmy tran/Automated-invoice-processing-system/Donuts/data_mhuy/converted_pngs',\n",
    "         label_file='/mnt/c/Users/Legion/Documents/jimmy tran/Automated-invoice-processing-system/Donuts/data_mhuy/mhuy_donut_rename.jsonl'),\n",
    "    dict(img_dir='/mnt/c/Users/Legion/Documents/jimmy tran/Automated-invoice-processing-system/Donuts/data_mcuong/converted_pngs',\n",
    "         label_file='/mnt/c/Users/Legion/Documents/jimmy tran/Automated-invoice-processing-system/Donuts/data_mcuong/mcuong_donut_rename.jsonl'),\n",
    "    dict(img_dir='/mnt/c/Users/Legion/Documents/jimmy tran/Automated-invoice-processing-system/Donuts/data_tnghia/converted_pngs',\n",
    "         label_file='/mnt/c/Users/Legion/Documents/jimmy tran/Automated-invoice-processing-system/Donuts/data_tnghia/tnghia_donut_rename.jsonl'),\n",
    "]\n",
    "\n",
    "BASE_MODEL = \"naver-clova-ix/donut-base\"   # Donut v1.0 base\n",
    "MAX_LEN    = 600                           # â‰¤ 600 tokens in Phase-1\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(BASE_MODEL)\n",
    "model     = VisionEncoderDecoderModel.from_pretrained(BASE_MODEL).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd780c",
   "metadata": {},
   "source": [
    "# 1 Â· Pre-processing helpers ðŸ”µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b41f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deskew â€¢ clahe â€¢ resize ------------------------------------------------------\n",
    "def deskew(img, limit=5):\n",
    "    g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(g, (9, 9), 0)\n",
    "    th = cv2.threshold(blur, 0, 255,\n",
    "                       cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    coords = np.column_stack(np.where(th > 0))\n",
    "    angle  = cv2.minAreaRect(coords)[-1]\n",
    "    angle  = -(90 + angle) if angle < -45 else -angle\n",
    "    if abs(angle) < limit:\n",
    "        (h, w) = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        img = cv2.warpAffine(img, M, (w, h),\n",
    "                             flags=cv2.INTER_CUBIC,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "    return img, angle\n",
    "\n",
    "def clahe(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    cl = cv2.createCLAHE(2.0, (8,8)).apply(l)\n",
    "    return cv2.cvtColor(cv2.merge((cl, a, b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def resize_long(img, target=1100):\n",
    "    h, w  = img.shape[:2]\n",
    "    scale = target / max(h, w)\n",
    "    if scale == 1:\n",
    "        return img\n",
    "    return cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def preprocess_one(path, show=False):\n",
    "    img, ang = deskew(cv2.imread(str(path)))\n",
    "    img      = clahe(img)\n",
    "    img      = resize_long(img)\n",
    "    if show:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"After preprocess (rotate {ang:.2f}Â°)\")\n",
    "        plt.axis('off'); plt.show()\n",
    "    out = path.with_suffix('.clean.jpg')\n",
    "    cv2.imwrite(str(out), img, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "    return out, ang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d75b7d",
   "metadata": {},
   "source": [
    "# 2 Â· Optional offline augmentation helper ðŸ”µ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c289b95",
   "metadata": {},
   "source": [
    "# 3 Â· Dataset builder (used by every phase) ðŸ”µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf2065e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(img_dir: str, label_file: str,\n",
    "                  max_len: int = MAX_LEN, augment: bool = False):\n",
    "    def proc_example(ex):\n",
    "        # load â†’ optional aug â†’ pixel-values tensor\n",
    "        img = cv2.imread(f\"{img_dir}/{ex['file_name']}\")\n",
    "        if augment:\n",
    "            img = AUG(image=img)[\"image\"].permute(1, 2, 0).numpy()\n",
    "        px  = processor.image_processor(img, return_tensors=\"pt\").pixel_values[0]\n",
    "\n",
    "        ids = processor.tokenizer(\n",
    "            ex[\"label\"], add_special_tokens=False,\n",
    "            max_length=max_len, truncation=True\n",
    "        ).input_ids\n",
    "        return {\"pixel_values\": px, \"labels\": ids}\n",
    "\n",
    "    ds = load_dataset(\"json\", data_files=label_file)[\"train\"]\n",
    "    return ds.map(proc_example, remove_columns=ds.column_names).with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e711fa",
   "metadata": {},
   "source": [
    "# 4 Â· Phase-1 curriculum training loop ðŸ”µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29855f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ© Phase-1 Â· step 1 | /mnt/c/Users/Legion/Documents/jimmy tran/Automated-invoice-processing-system/Donuts/data_mhuy/converted_pngs | 14:02:34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f3a737687440fdb9349612afbd7cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7d6d5b2f9c4dfd8027bf6ba54f86c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()          # keep console clean\n",
    "\n",
    "for step_idx, phase in enumerate(PHASES, 1):\n",
    "    print(f\"\\nðŸŸ© Phase-1 Â· step {step_idx} | {phase['img_dir']} | \"\n",
    "          f\"{datetime.now().strftime('%H:%M:%S')}\")\n",
    "    ds        = build_dataset(phase[\"img_dir\"], phase[\"label_file\"])\n",
    "    out_dir   = f\"./donut_phase1_step{step_idx}\"\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir                   = out_dir,\n",
    "        per_device_train_batch_size  = 1,\n",
    "        gradient_accumulation_steps  = 8,\n",
    "        num_train_epochs             = 3,\n",
    "        learning_rate                = 2e-5,\n",
    "        warmup_steps                 = 200,\n",
    "        fp16                         = True,\n",
    "        logging_steps                = 50,\n",
    "        save_total_limit             = 2,\n",
    "        report_to                    = \"none\",\n",
    "        resume_from_checkpoint       = os.path.isdir(out_dir),\n",
    "    )\n",
    "    Trainer(model=model, args=args, train_dataset=ds).train()\n",
    "    model.save_pretrained(f\"{out_dir}/checkpoint_final\")  # explicit save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf8c3f",
   "metadata": {},
   "source": [
    "## After this loop finishes, your final Phase-1 weights are in:\n",
    "\n",
    "./donut_phase1_step3/checkpoint_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc0268",
   "metadata": {},
   "source": [
    "# 5 Â· Utility & inference helpers ðŸ”µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a227b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€ regex/amount/date helpers (unchanged) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MST = re.compile(r'\\b\\d{10}(?:\\d{3})?\\b')\n",
    "\n",
    "def normalize_amount(txt):\n",
    "    txt = txt.replace(',', '.').replace(' ', '')\n",
    "    m   = re.findall(r'[\\d\\.]+', txt)\n",
    "    return m[0].replace('.', '') if m else txt\n",
    "\n",
    "def fix_taxcode(code):\n",
    "    m = MST.search(code or '')\n",
    "    return m.group(0) if m else code\n",
    "\n",
    "def parse_date(txt):\n",
    "    dt = dateparser.parse(txt, settings={'DATE_ORDER':'DMY'})\n",
    "    return dt.strftime('%Y-%m-%d') if dt else txt\n",
    "\n",
    "# â”€â”€â”€â”€â”€ single-invoice prediction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@torch.no_grad()\n",
    "def predict_one(img_path: str, max_len: int = 512):\n",
    "    pv  = processor.image_processor(img_path,\n",
    "                                    return_tensors='pt').pixel_values.to(DEVICE)\n",
    "    out = model.generate(pv, max_length=max_len)\n",
    "    return processor.batch_decode(out, skip_special_tokens=False)[0]\n",
    "\n",
    "# demo (comment out until training finished)\n",
    "# print(predict_one(\"sample_invoice.jpg\")[:400])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
